{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "wordslist = []\n",
    "wordVectors = np.zeros((1193514, 25), dtype='float32')\n",
    "id_dict = {}\n",
    "sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read stanford glove (25d) (try out with multiple dimensions after )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "with open(\"./stanfordglove/glove.twitter.27B.25d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        wordslist.append(word)\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector\n",
    "        if(vector.shape == (25,)):\n",
    "            wordVectors[index] = vector\n",
    "            embeddings_index[word]= vector\n",
    "        id_dict[word]= index\n",
    "        index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = []\n",
    "pos_sentences = []\n",
    "with open('train_pos.txt') as file:\n",
    "    for line in file:\n",
    "        sentences.append(line)\n",
    "        sentence = np.zeros(60, dtype='int32')\n",
    "        count = 0\n",
    "        for val in line.strip().split():\n",
    "            idx = id_dict.get(val, -1)\n",
    "            if(idx != -1):\n",
    "                sentence[count] = idx \n",
    "                count+=1\n",
    "        pos_train.append(np.array(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_arr = np.array(pos_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train = []\n",
    "with open('train_neg.txt') as file:\n",
    "    for line in file:\n",
    "        sentences.append(line)\n",
    "        sentence = np.zeros(60, dtype='int32')\n",
    "        count = 0\n",
    "        for val in line.strip().split():\n",
    "            idx = id_dict.get(val, -1)\n",
    "            if(idx != -1):\n",
    "                sentence[count] = idx\n",
    "                count+=1\n",
    "        neg_train.append(np.array(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train_arr= np.array(neg_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "poslabels = np.repeat(1,pos_train_arr.shape[0])\n",
    "neglabels = np.repeat(0,neg_train_arr.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.append(poslabels,neglabels)\n",
    "training_set = np.concatenate((pos_train_arr, neg_train_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## put data into dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = pd.DataFrame({'text':sentences,'sentiment':labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## standarize  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "#data = data[data.sentiment != \"Neutral\"]\n",
    "data_new['text'] = data_new['text'].apply(lambda x: x.lower())\n",
    "data_new['text'] = data_new['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "\n",
    "print(data_new[ data_new['sentiment'] == 1].size)\n",
    "print(data_new[ data_new['sentiment'] == 0].size)\n",
    "\n",
    "for idx,row in data_new.iterrows():\n",
    "    row[0] = row[0].replace('rt',' ')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many unique words ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = {}\n",
    "for sentence in data_new['text'].values:\n",
    "    for word in sentence.split():\n",
    "        s[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107090"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = len(s) \n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data_new['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data_new['text'].values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193513 word vectors.\n",
      "Found 106936 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build embedding matrix from data to glove embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106937\n"
     ]
    }
   ],
   "source": [
    "num_words = min(max_fatures, len(word_index)) + 1\n",
    "print(num_words)\n",
    "\n",
    "embedding_dim = 25\n",
    "\n",
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_index.items():\n",
    "    if i > max_fatures:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107090"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_fatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'ConfigProto'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-215b74ef11a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrewriter_config_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewriter_config_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRewriterConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOFF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewrite_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_optimization\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0moff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'ConfigProto'"
     ]
    }
   ],
   "source": [
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "config = tf.ConfigProto()\n",
    "off = rewriter_config_pb2.RewriterConfig.OFF\n",
    "config.graph_options.rewrite_options.memory_optimization  = off\n",
    "return tf.Session(config=config)\n",
    "\n",
    "#tf.config.optimizer.set_experimental_options({'arithmetic_optimization': False})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 47, 25)            2673425   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_11 (Spatia (None, 47, 25)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 392)               348096    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 786       \n",
      "=================================================================\n",
      "Total params: 3,022,307\n",
      "Trainable params: 3,022,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import Constant\n",
    "embed_dim = 25\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                    input_length=X.shape[1],\n",
    "                    trainable=True))\n",
    "\n",
    "\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(Bidirectional(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = pd.get_dummies(data_new['sentiment']).values\n",
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134000, 47) (134000, 2)\n",
      "(66000, 47) (66000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "4188/4188 - 780s - loss: 0.4947 - accuracy: 0.7403\n",
      "Epoch 2/7\n",
      "4188/4188 - 955s - loss: 0.4176 - accuracy: 0.8004\n",
      "Epoch 3/7\n",
      "4188/4188 - 940s - loss: 0.3902 - accuracy: 0.8174\n",
      "Epoch 4/7\n",
      "4188/4188 - 828s - loss: 0.3732 - accuracy: 0.8282\n",
      "Epoch 5/7\n",
      "4188/4188 - 837s - loss: 0.3564 - accuracy: 0.8372\n",
      "Epoch 6/7\n",
      "4188/4188 - 803s - loss: 0.3462 - accuracy: 0.8428\n",
      "Epoch 7/7\n",
      "4188/4188 - 956s - loss: 0.3341 - accuracy: 0.8495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ad43d6630>"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 7, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016/2016 - 46s - loss: 0.3653 - accuracy: 0.8327\n",
      "score: 0.37\n",
      "acc: 0.83\n"
     ]
    }
   ],
   "source": [
    "validation_size = 1500\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model25dglove.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model25dglove.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now predict sentiment in competition data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = []\n",
    "with open('test_data.txt') as file:\n",
    "    for line in file:\n",
    "        tweet= line.split(\",\")[0]\n",
    "        tweet = line[len(tweet)+1:]\n",
    "        test_sentences.append(tweet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sea doo pro sea scooter ( sports with the port...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;user&gt; shucks well i work all week so now i ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i cant stay away from bug thats my baby\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; no ma'am ! ! ! lol im perfectly fine an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>whenever i fall asleep watching the tv , i alw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>had a nice time w / my friend lastnite\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>&lt;user&gt; no it's not ! please stop !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>not without my daughter ( dvd two-time oscar (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>&lt;user&gt; have fun in class sweetcheeks\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>making a r . e . a . l . difference . ( get r ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet\n",
       "0     sea doo pro sea scooter ( sports with the port...\n",
       "1     <user> shucks well i work all week so now i ca...\n",
       "2             i cant stay away from bug thats my baby\\n\n",
       "3     <user> no ma'am ! ! ! lol im perfectly fine an...\n",
       "4     whenever i fall asleep watching the tv , i alw...\n",
       "...                                                 ...\n",
       "9995           had a nice time w / my friend lastnite\\n\n",
       "9996               <user> no it's not ! please stop !\\n\n",
       "9997  not without my daughter ( dvd two-time oscar (...\n",
       "9998             <user> have fun in class sweetcheeks\\n\n",
       "9999  making a r . e . a . l . difference . ( get r ...\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competition_data = pd.DataFrame({\"tweet\":test_sentences})\n",
    "competition_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data[data.sentiment != \"Neutral\"]\n",
    "competition_data['tweet'] = competition_data['tweet'].apply(lambda x: x.lower())\n",
    "competition_data['tweet'] = competition_data['tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       sea doo pro sea scooter  sports with the porta...\n",
       "1       user shucks well i work all week so now i cant...\n",
       "2               i cant stay away from bug thats my baby\\n\n",
       "3       user no maam    lol im perfectly fine and not ...\n",
       "4       whenever i fall asleep watching the tv  i alwa...\n",
       "                              ...                        \n",
       "9995              had a nice time w  my friend lastnite\\n\n",
       "9996                      user no its not  please stop \\n\n",
       "9997    not without my daughter  dvd twotime oscar  r ...\n",
       "9998                 user have fun in class sweetcheeks\\n\n",
       "9999    making a r  e  a  l  difference   get r  e  a ...\n",
       "Name: tweet, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competition_data['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "twts = tokenizer.texts_to_sequences(competition_data['tweet'].values)\n",
    "\n",
    "twts = pad_sequences(twts, maxlen=47, dtype='int32', value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   812,     8,     6],\n",
       "       [    0,     0,     0, ...,    14,    28, 10128],\n",
       "       [    0,     0,     0, ...,    83,     9,   179],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,     4,    80,     6],\n",
       "       [    0,     0,     0, ...,    14,   355, 79501],\n",
       "       [    0,     0,     0, ...,    62,   199,     6]], dtype=int32)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 8s\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(twts,batch_size=32,verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_arr=[]\n",
    "id_count= 1\n",
    "for pred in preds:\n",
    "    if(np.argmax(pred ) == 0):\n",
    "        output_arr.append([id_count, -1])\n",
    "    elif (np.argmax(pred ) == 1):\n",
    "        output_arr.append([id_count, 1])\n",
    "    id_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(np.array(output_arr))\n",
    "output_df.columns=[\"Id\", \"Prediction\"]\n",
    "output_df.set_index('Id', inplace=True)\n",
    "output_df.to_csv(\"Predictions2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9173, 180, 1507, 11, 151, 12, 79, 1811, 79, 29, 11, 151]]\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0 9173  180 1507   11  151   12   79 1811   79   29   11  151]]\n",
      "1/1 - 0s\n",
      "[9.9978906e-01 2.1099388e-04]\n",
      "negative\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
